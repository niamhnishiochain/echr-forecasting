{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Everything\n",
    "\n",
    "### Things to try:\n",
    "1. Tune the different classifiers\n",
    "2. Tweaking the pipeline\n",
    "3. Tweaking the tfidf\n",
    "4. Doing something instead of the tfidf? Eg. BERT Legal Tokenizer?\n",
    "5. After the splits into sections do some preprocessing on the text? (doing it in the tokenizer for now)\n",
    "6. Class imbalance\n",
    "7. Use the holdout data (test20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all necessary imports\n",
    "#basics\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "#text stuff\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#sklearn stuff\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "#models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this \n",
    "def extract_text(starts, ends, cases, violation):\n",
    "    facts = []\n",
    "    D = []\n",
    "    years = []\n",
    "    for case in cases:\n",
    "        contline = ''\n",
    "        year = 0\n",
    "        with open(case, 'r', encoding=\"mbcs\") as f:\n",
    "            for line in f:\n",
    "                dat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "                if dat != None:\n",
    "                    year = int(dat.group(2))\n",
    "                    break\n",
    "            if year>0:\n",
    "                years.append(year)\n",
    "                wr = 0\n",
    "                for line in f:\n",
    "                    if wr == 0:\n",
    "                        if re.search(starts, line) != None:\n",
    "                            wr = 1\n",
    "                    if wr == 1 and re.search(ends, line) == None:\n",
    "                        contline += line\n",
    "                        contline += '\\n'\n",
    "                    elif re.search(ends, line) != None:\n",
    "                        break\n",
    "                facts.append(contline)\n",
    "    for i in range(len(facts)):\n",
    "        D.append((facts[i], violation, years[i])) \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parts(train_path, violation, part): #extract text from different parts\n",
    "    cases = glob.glob(train_path)\n",
    "\n",
    "    facts = []\n",
    "    D = []\n",
    "    years = []\n",
    "    \n",
    "    if part == 'relevant_law': #seprarte extraction for relevant law\n",
    "        for case in cases:\n",
    "            year = 0\n",
    "            contline = ''\n",
    "            with open(case, 'r', encoding=\"mbcs\") as f:\n",
    "                for line in f:\n",
    "                    dat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "                    if dat != None:\n",
    "                        year = int(dat.group(2))\n",
    "                        break\n",
    "                if year> 0:\n",
    "                    years.append(year)\n",
    "                    wr = 0\n",
    "                    for line in f:\n",
    "                        if wr == 0:\n",
    "                            if re.search('RELEVANT', line) != None:\n",
    "                                wr = 1\n",
    "                        if wr == 1 and re.search('THE LAW', line) == None and re.search('PROCEEDINGS', line) == None:\n",
    "                            contline += line\n",
    "                            contline += '\\n'\n",
    "                        elif re.search('THE LAW', line) != None or re.search('PROCEEDINGS', line) != None:\n",
    "                            break\n",
    "                    facts.append(contline)\n",
    "        for i in range(len(facts)):\n",
    "            D.append((facts[i], violation, years[i]))\n",
    "        \n",
    "    if part == 'facts':\n",
    "        starts = 'THE FACTS'\n",
    "        ends ='THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'circumstances':\n",
    "        starts = 'CIRCUMSTANCES'\n",
    "        ends ='RELEVANT'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'procedure':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends ='THE FACTS'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'procedure+facts':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends ='THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'facts+circumstances':\n",
    "        starts = 'THE FACTS'\n",
    "        ends = 'RELEVANT'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'facts+circumstances+procedure':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends = 'THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [RandomForestClassifier(), LinearSVC(), XGBClassifier()]\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cross_val(Xtrain, Ytrain, Xtest, Ytest, vec, models): # model cross-validation and evaluation\n",
    "    print('***10-fold cross-validation***')\n",
    "    for i, model in enumerate(models):\n",
    "        pipeline = Pipeline([\n",
    "            ('features', FeatureUnion([vec],)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "        #cross validate and find the best model\n",
    "        best_model = HalvingGridSearchCV(pipeline, params, scoring = 'roc_auc', cv = 3, verbose = 3)\n",
    "       \n",
    "        #in sample predictions\n",
    "        print('***IN SAMPLE***')\n",
    "        best_model.fit(Xtrain, Ytrain)\n",
    "        Ypredict_in = best_model.predict(Xtrain)\n",
    "        evaluate(Ytrain, Ypredict_in)\n",
    "        \n",
    "        #out of sample\n",
    "        print('***OUT OF SAMPLE***')\n",
    "        Ypredict_out = best_model.predict(Xtest)\n",
    "        evaluate(Ytest, Ypredict_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Ytest, Ypredict): #evaluate the model (accuracy, precision, recall, f-score, confusion matrix)\n",
    "        print('Accuracy:', accuracy_score(Ytest, Ypredict) )\n",
    "        print('\\nClassification report:\\n', classification_report(Ytest, Ypredict))\n",
    "        print('\\nCR:', precision_recall_fscore_support(Ytest, Ypredict, average='macro'))\n",
    "        print('\\nConfusion matrix:\\n', confusion_matrix(Ytest, Ypredict), '\\n\\n_______________________\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(part, vec): #run tests\n",
    "    \n",
    "    print('Trained on *' + part + '* part of the cases')\n",
    "    \n",
    "    v = extract_parts(path+'train/'+article+'/violation/*.txt', 'violation', part)\n",
    "    nv = extract_parts(path+'train/'+article+'/non-violation/*.txt', 'non-violation', part)\n",
    "    trainset =v+nv\n",
    "    shuffle(trainset)\n",
    "\n",
    "    Xtrain = [i[0] for i in trainset]\n",
    "    Ytrain = [i[1] for i in trainset]\n",
    "    \n",
    "    #test set\n",
    "    test_nv = extract_parts(path + '/test_violations/'+article+'/*.txt', 'non-violation', part)\n",
    "    test_v = extract_parts(path + '/test_violations/'+article+'/*.txt', 'violation', part)\n",
    "    \n",
    "    testset = test_nv+test_v\n",
    "    shuffle(testset)\n",
    "    Xtest = [i[0] for i in testset]\n",
    "    Ytest = [i[1] for i in testset]\n",
    "    \n",
    "    print('Training on', Ytrain.count('violation'),'+', Ytrain.count('non-violation'), '=', Ytrain.count('violation') + Ytrain.count('non-violation'), 'cases', '\\nCases available for testing(violation):', Ytest_v.count('violation'))\n",
    "    #train_model_test(Xtrain, Ytrain, Xtest_v, Ytest_v, vec, c)\n",
    "    train_model_cross_val(Xtrain, Ytrain, Xtest, Ytest, vec, models) #use for cross-validation\n",
    "    #print(len(v[1]), len(nv[1]), len(Xtrain[1]), len(Ytrain))\n",
    "    return v, nv, Xtrain, Ytrain, Xtest, Ytest, trainset, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on *facts* part of the cases\n",
      "Training on 278 + 279 = 557 cases \n",
      "Cases available for testing(violation): 833\n",
      "***10-fold cross-validation***\n",
      "***IN SAMPLE***\n",
      "n_iterations: 1\n",
      "n_required_iterations: 1\n",
      "n_possible_iterations: 1\n",
      "min_resources_: 557\n",
      "max_resources_: 557\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1\n",
      "n_resources: 557\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END ..............., score=(train=1.000, test=0.827) total time=   7.0s\n",
      "[CV 2/3] END ..............., score=(train=1.000, test=0.844) total time=   6.9s\n",
      "[CV 3/3] END ..............., score=(train=1.000, test=0.800) total time=   6.9s\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-violation       1.00      1.00      1.00       279\n",
      "    violation       1.00      1.00      1.00       278\n",
      "\n",
      "     accuracy                           1.00       557\n",
      "    macro avg       1.00      1.00      1.00       557\n",
      " weighted avg       1.00      1.00      1.00       557\n",
      "\n",
      "\n",
      "CR: (1.0, 1.0, 1.0, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[279   0]\n",
      " [  0 278]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      "***OUT OF SAMPLE***\n",
      "Accuracy: 0.5\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-violation       0.50      0.21      0.30       833\n",
      "    violation       0.50      0.79      0.61       833\n",
      "\n",
      "     accuracy                           0.50      1666\n",
      "    macro avg       0.50      0.50      0.45      1666\n",
      " weighted avg       0.50      0.50      0.45      1666\n",
      "\n",
      "\n",
      "CR: (0.5, 0.5, 0.4545315905423917, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[176 657]\n",
      " [176 657]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      "***IN SAMPLE***\n",
      "n_iterations: 1\n",
      "n_required_iterations: 1\n",
      "n_possible_iterations: 1\n",
      "min_resources_: 557\n",
      "max_resources_: 557\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1\n",
      "n_resources: 557\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END ..............., score=(train=1.000, test=0.850) total time=   6.2s\n",
      "[CV 2/3] END ..............., score=(train=1.000, test=0.862) total time=   6.0s\n",
      "[CV 3/3] END ..............., score=(train=1.000, test=0.851) total time=   6.1s\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-violation       1.00      1.00      1.00       279\n",
      "    violation       1.00      1.00      1.00       278\n",
      "\n",
      "     accuracy                           1.00       557\n",
      "    macro avg       1.00      1.00      1.00       557\n",
      " weighted avg       1.00      1.00      1.00       557\n",
      "\n",
      "\n",
      "CR: (1.0, 1.0, 1.0, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[279   0]\n",
      " [  0 278]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      "***OUT OF SAMPLE***\n",
      "Accuracy: 0.5\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-violation       0.50      0.21      0.29       833\n",
      "    violation       0.50      0.79      0.61       833\n",
      "\n",
      "     accuracy                           0.50      1666\n",
      "    macro avg       0.50      0.50      0.45      1666\n",
      " weighted avg       0.50      0.50      0.45      1666\n",
      "\n",
      "\n",
      "CR: (0.5, 0.5, 0.4528627778752461, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[172 661]\n",
      " [172 661]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      "***IN SAMPLE***\n",
      "n_iterations: 1\n",
      "n_required_iterations: 1\n",
      "n_possible_iterations: 1\n",
      "min_resources_: 557\n",
      "max_resources_: 557\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1\n",
      "n_resources: 557\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[14:41:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/3] END ..............., score=(train=1.000, test=0.786) total time=   9.9s\n",
      "[14:41:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/3] END ..............., score=(train=1.000, test=0.798) total time=   9.8s\n",
      "[14:41:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/3] END ..............., score=(train=1.000, test=0.809) total time=  10.2s\n",
      "[14:41:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-violation       1.00      1.00      1.00       279\n",
      "    violation       1.00      1.00      1.00       278\n",
      "\n",
      "     accuracy                           1.00       557\n",
      "    macro avg       1.00      1.00      1.00       557\n",
      " weighted avg       1.00      1.00      1.00       557\n",
      "\n",
      "\n",
      "CR: (1.0, 1.0, 1.0, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[279   0]\n",
      " [  0 278]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      "***OUT OF SAMPLE***\n",
      "Accuracy: 0.5\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-violation       0.50      0.22      0.31       833\n",
      "    violation       0.50      0.78      0.61       833\n",
      "\n",
      "     accuracy                           0.50      1666\n",
      "    macro avg       0.50      0.50      0.46      1666\n",
      " weighted avg       0.50      0.50      0.46      1666\n",
      "\n",
      "\n",
      "CR: (0.5, 0.5, 0.45854114360514003, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[186 647]\n",
      " [186 647]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Path to the data\n",
    "    path = 'C:\\\\Users\\\\35387\\\\Dropbox\\\\Documents\\\\Forecasting\\\\Final_Project\\\\data\\\\'\n",
    "    \n",
    "    articles = ['Article3'] # 'Article3', 'Article5', 'Article6', 'Article8', 'Article10', 'Article11', 'Article13', 'Article14']\n",
    "    for part in ['facts']: #, 'facts+circumstances', 'facts+circumstances+procedure']:\n",
    "        for article in articles: #the parameters were determined using grid-search\n",
    "                vec = ('wordvec', TfidfVectorizer(analyzer = 'word', ngram_range = (3,4), binary = False, lowercase = True, min_df = 2, norm = 'l2', stop_words = None, use_idf = True))\n",
    "                run_pipeline(part, vec) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
