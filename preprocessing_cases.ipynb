{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Everything\n",
    "\n",
    "### Things to try:\n",
    "1. Tune the different classifiers\n",
    "2. Tweaking the pipeline\n",
    "3. Tweaking the tfidf\n",
    "4. Doing something instead of the tfidf? Eg. BERT Legal Tokenizer?\n",
    "5. After the splits into sections do some preprocessing on the text? (doing it in the tokenizer for now)\n",
    "6. Class imbalance\n",
    "7. Use the holdout data (test20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all necessary imports\n",
    "#basics\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from random import shuffle\n",
    "import warnings\n",
    "\n",
    "#text stuff\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#sklearn stuff\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this \n",
    "def extract_text(starts, ends, cases, violation):\n",
    "    facts = []\n",
    "    D = []\n",
    "    years = []\n",
    "    for case in cases:\n",
    "        contline = ''\n",
    "        year = 0\n",
    "        with open(case, 'r', encoding=\"mbcs\") as f:\n",
    "            for line in f:\n",
    "                dat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "                if dat != None:\n",
    "                    year = int(dat.group(2))\n",
    "                    break\n",
    "            if year>0:\n",
    "                years.append(year)\n",
    "                wr = 0\n",
    "                for line in f:\n",
    "                    if wr == 0:\n",
    "                        if re.search(starts, line) != None:\n",
    "                            wr = 1\n",
    "                    if wr == 1 and re.search(ends, line) == None:\n",
    "                        contline += line\n",
    "                        contline += '\\n'\n",
    "                    elif re.search(ends, line) != None:\n",
    "                        break\n",
    "                facts.append(contline)\n",
    "    for i in range(len(facts)):\n",
    "        D.append((facts[i], violation, years[i])) \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parts(train_path, violation, part): #extract text from different parts\n",
    "    cases = glob.glob(train_path)\n",
    "\n",
    "    facts = []\n",
    "    D = []\n",
    "    years = []\n",
    "    \n",
    "    if part == 'relevant_law': #seprarte extraction for relevant law\n",
    "        for case in cases:\n",
    "            year = 0\n",
    "            contline = ''\n",
    "            with open(case, 'r', encoding=\"mbcs\") as f:\n",
    "                for line in f:\n",
    "                    dat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "                    if dat != None:\n",
    "                        year = int(dat.group(2))\n",
    "                        break\n",
    "                if year> 0:\n",
    "                    years.append(year)\n",
    "                    wr = 0\n",
    "                    for line in f:\n",
    "                        if wr == 0:\n",
    "                            if re.search('RELEVANT', line) != None:\n",
    "                                wr = 1\n",
    "                        if wr == 1 and re.search('THE LAW', line) == None and re.search('PROCEEDINGS', line) == None:\n",
    "                            contline += line\n",
    "                            contline += '\\n'\n",
    "                        elif re.search('THE LAW', line) != None or re.search('PROCEEDINGS', line) != None:\n",
    "                            break\n",
    "                    facts.append(contline)\n",
    "        for i in range(len(facts)):\n",
    "            D.append((facts[i], violation, years[i]))\n",
    "        \n",
    "    if part == 'facts':\n",
    "        starts = 'THE FACTS'\n",
    "        ends ='THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'circumstances':\n",
    "        starts = 'CIRCUMSTANCES'\n",
    "        ends ='RELEVANT'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'procedure':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends ='THE FACTS'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'procedure+facts':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends ='THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'facts+circumstances':\n",
    "        starts = 'THE FACTS'\n",
    "        ends = 'RELEVANT'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'facts+circumstances+procedure':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends = 'THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'loss', 'max_iter', 'multi_class', 'penalty', 'random_state', 'tol', 'verbose'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "svc.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [RandomForestClassifier(class_weight = 'balanced')]\n",
    "\n",
    "#model specific params\n",
    "'''\n",
    "svc_params = {'classifier__class_weight':[None, 'balanced'],\n",
    "              'classifier__probability':[True],\n",
    "             # 'classifier__C':[0.5, 1],\n",
    "              'classifier__kernel':['linear', 'poly', 'rbf'],\n",
    "              'classifier__random_state': [SEED]}\n",
    "\n",
    "randomforest_params = {'classifier__max_features': range(4, 10),\n",
    "                  'classifier__n_estimators': [50, 100],\n",
    "                  'classifier__max_depth': range(3, 6),\n",
    "                  'classifier__min_samples_leaf': range(5, 10),\n",
    "                  'classifier__random_state': [SEED]}\n",
    "\n",
    "'''\n",
    "\n",
    "xgb_params = {'classifier__objective':['binary:logistic'],\n",
    "              'classifier__eval_metric':['auc'],\n",
    "                  'classifier__learning_rate': [0.05, 1],\n",
    "                  'classifier__max_depth': range(3, 10),\n",
    "                  'classifier__lambda':[0.5, 1.5],\n",
    "              'classifier__use_label_encoder':[False],\n",
    "                  'classifier__seed': [SEED]}\n",
    "\n",
    "#params = [svc_params, randomforest_params, \n",
    "#param = [xgb_params]\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cross_val(Xtrain, Ytrain, Xtest, Ytest, vec, models): # model cross-validation and evaluation\n",
    "    print('***10-fold cross-validation***')\n",
    "    for i, model in enumerate(models):\n",
    "        pipeline = Pipeline([\n",
    "            ('features', FeatureUnion([vec],)),\n",
    "            ('classifier', LinearSVC())\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        #cross validate and find the best model\n",
    "        print('***CV IN PROGRESS***')\n",
    "        best_model = HalvingGridSearchCV(pipeline, params, scoring = 'roc_auc', cv = 3, verbose = 3) #Halving\n",
    "       \n",
    "        #in sample predictions\n",
    "        print('***IN SAMPLE***')\n",
    "        best_model.fit(Xtrain, Ytrain)\n",
    "        Ypredict_in = best_model.predict(Xtrain)\n",
    "        print(model)\n",
    "        evaluate(Ytrain, Ypredict_in)\n",
    "        \n",
    "        #out of sample\n",
    "        print('***OUT OF SAMPLE***')     \n",
    "        Ypredict_out = best_model.predict(Xtest)\n",
    "        print(model)\n",
    "        evaluate(Ytest, Ypredict_out)\n",
    "        \n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Ytest, Ypredict): #evaluate the model (accuracy, precision, recall, f-score, confusion matrix)\n",
    "        print('Accuracy:', accuracy_score(Ytest, Ypredict))\n",
    "        print('\\nClassification report:\\n', classification_report(Ytest, Ypredict))\n",
    "        print('\\nCR:', precision_recall_fscore_support(Ytest, Ypredict, average='macro'))\n",
    "        print('\\nConfusion matrix:\\n', confusion_matrix(Ytest, Ypredict), '\\n\\n_______________________\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(part, vec): #run tests\n",
    "    \n",
    "    print('Trained on *' + part + '* part of the cases')\n",
    "    \n",
    "    v = extract_parts(path+'train/'+article+'/violation/*.txt', 'violation', part)\n",
    "    nv = extract_parts(path+'train/'+article+'/non-violation/*.txt', 'non-violation', part)\n",
    "    trainset =v+nv\n",
    "    shuffle(trainset)\n",
    "\n",
    "    Xtrain = [i[0] for i in trainset]\n",
    "    Ytrain = [i[1] for i in trainset]\n",
    "    \n",
    "    #test set\n",
    "    test_nv = extract_parts(path + '/test_violations/'+article+'/*.txt', 'non-violation', part)\n",
    "    test_v = extract_parts(path + '/test_violations/'+article+'/*.txt', 'violation', part)\n",
    "    \n",
    "    testset = test_nv+test_v\n",
    "    shuffle(testset)\n",
    "    Xtest = [i[0] for i in testset]\n",
    "    Ytest = [i[1] for i in testset]\n",
    "    \n",
    "    print(article)\n",
    "    print('Training on', Ytrain.count('violation'), 'violations', '+', Ytrain.count('non-violation'), 'non-violations', '=', Ytrain.count('violation') + Ytrain.count('non-violation'), 'cases')\n",
    "    print('Testing on', Ytest.count('violation'), 'violations','+', Ytest.count('non-violation'), 'non-violations', '=', Ytest.count('violation') + Ytest.count('non-violation'), 'cases')\n",
    "       \n",
    "    best_model = train_model_cross_val(Xtrain, Ytrain, Xtest, Ytest, vec, models) \n",
    "    \n",
    "    return v, nv, Xtrain, Ytrain, Xtest, Ytest, trainset, testset, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(part, vec): #run tests\n",
    "    \n",
    "    print('Trained on *' + part + '* part of the cases')\n",
    "    \n",
    "    v = extract_parts(path+'train/'+article+'/violation/*.txt', 'violation', part)\n",
    "    nv = extract_parts(path+'train/'+article+'/non-violation/*.txt', 'non-violation', part)\n",
    "    test_nv = extract_parts(path + '/test_violations/'+article+'/*.txt', 'non-violation', part)\n",
    "    test_v = extract_parts(path + '/test_violations/'+article+'/*.txt', 'violation', part)\n",
    "    data =v+nv+test_nv+test_v\n",
    "    shuffle(data)\n",
    "\n",
    "    X = [i[0] for i in data]\n",
    "    y = [i[1] for i in data]\n",
    "    \n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "    \n",
    "    print(article)\n",
    "    print('Training on', Ytrain.count('violation'), 'violations', '+', Ytrain.count('non-violation'), 'non-violations', '=', Ytrain.count('violation') + Ytrain.count('non-violation'), 'cases')\n",
    "    print('Testing on', Ytest.count('violation'), 'violations','+', Ytest.count('non-violation'), 'non-violations', '=', Ytest.count('violation') + Ytest.count('non-violation'), 'cases')\n",
    "       \n",
    "    best_model = train_model_cross_val(Xtrain, Ytrain, Xtest, Ytest, vec, models) \n",
    "    \n",
    "    return v, nv, Xtrain, Ytrain, Xtest, Ytest, trainset, testset, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on *facts+circumstances+procedure* part of the cases\n",
      "Article2\n",
      "Training on 308 violations + 324 non-violations = 632 cases\n",
      "Testing on 143 violations + 128 non-violations = 271 cases\n",
      "***10-fold cross-validation***\n",
      "***CV IN PROGRESS***\n",
      "***IN SAMPLE***\n",
      "n_iterations: 1\n",
      "n_required_iterations: 1\n",
      "n_possible_iterations: 1\n",
      "min_resources_: 632\n",
      "max_resources_: 632\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1\n",
      "n_resources: 632\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END ..............., score=(train=0.922, test=0.208) total time=   9.0s\n",
      "[CV 2/3] END ..............., score=(train=0.925, test=0.193) total time=   8.7s\n",
      "[CV 3/3] END ..............., score=(train=0.926, test=0.178) total time=  11.2s\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "warnings.filterwarnings('ignore')\n",
    "if __name__ == \"__main__\":\n",
    "    #Path to the data\n",
    "    path = 'C:\\\\Users\\\\35387\\\\Dropbox\\\\Documents\\\\Forecasting\\\\Final_Project\\\\data\\\\'\n",
    "    \n",
    "    articles = ['Article2'] #'Article3', 'Article4' 'Article5', 'Article6', 'Article8', 'Article10', 'Article11', 'Article13', 'Article14']\n",
    "    for part in ['facts+circumstances+procedure']: # 'facts' 'facts+circumstances+procedure']:\n",
    "        for article in articles: #the parameters were determined using grid-search\n",
    "                vec = ('wordvec', TfidfVectorizer(analyzer = 'word', ngram_range = (3,4), binary = False, lowercase = True, min_df = 2, norm = 'l2', stop_words = None, use_idf = True))\n",
    "                v, nv, Xtrain, Ytrain, Xtest, Ytest, trainset, testset, best_model = run_pipeline(part, vec) \n",
    "                \n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "8\n",
      "8\n",
      "6\n",
      "6\n",
      "8\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(v))\n",
    "print(len(nv))\n",
    "print(len(Xtrain))\n",
    "print(len(Ytrain))\n",
    "print(len(Xtest))\n",
    "print(len(Ytest)) \n",
    "print(len(trainset))\n",
    "print(len(testset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
