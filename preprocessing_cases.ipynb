{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Everything\n",
    "\n",
    "### Things to try:\n",
    "1. Different classifiers \n",
    "2. Tweaking the pipeline\n",
    "3. Tweaking the tfidf\n",
    "4. Doing something instead of the tfidf? Eg. BERT Legal Tokenizer?\n",
    "5. After the splits into sections do some preprocessing on the text? (doing it in the tokenizer for now)\n",
    "6. Class imbalance\n",
    "7. Use the holdout data (test20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all necessary imports\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import glob,re, os, sys, random\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "import os\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this \n",
    "def extract_text(starts, ends, cases, violation):\n",
    "    facts = []\n",
    "    D = []\n",
    "    years = []\n",
    "    for case in cases:\n",
    "        contline = ''\n",
    "        year = 0\n",
    "        with open(case, 'r', encoding=\"mbcs\") as f:\n",
    "            for line in f:\n",
    "                dat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "                if dat != None:\n",
    "                    year = int(dat.group(2))\n",
    "                    break\n",
    "            if year>0:\n",
    "                years.append(year)\n",
    "                wr = 0\n",
    "                for line in f:\n",
    "                    if wr == 0:\n",
    "                        if re.search(starts, line) != None:\n",
    "                            wr = 1\n",
    "                    if wr == 1 and re.search(ends, line) == None:\n",
    "                        contline += line\n",
    "                        contline += '\\n'\n",
    "                    elif re.search(ends, line) != None:\n",
    "                        break\n",
    "                facts.append(contline)\n",
    "    for i in range(len(facts)):\n",
    "        D.append((facts[i], violation, years[i])) \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parts(train_path, violation, part): #extract text from different parts\n",
    "    cases = glob.glob(train_path)\n",
    "\n",
    "    facts = []\n",
    "    D = []\n",
    "    years = []\n",
    "    \n",
    "    if part == 'relevant_law': #seprarte extraction for relevant law\n",
    "        for case in cases:\n",
    "            year = 0\n",
    "            contline = ''\n",
    "            with open(case, 'r', encoding=\"mbcs\") as f:\n",
    "                for line in f:\n",
    "                    dat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "                    if dat != None:\n",
    "                        year = int(dat.group(2))\n",
    "                        break\n",
    "                if year> 0:\n",
    "                    years.append(year)\n",
    "                    wr = 0\n",
    "                    for line in f:\n",
    "                        if wr == 0:\n",
    "                            if re.search('RELEVANT', line) != None:\n",
    "                                wr = 1\n",
    "                        if wr == 1 and re.search('THE LAW', line) == None and re.search('PROCEEDINGS', line) == None:\n",
    "                            contline += line\n",
    "                            contline += '\\n'\n",
    "                        elif re.search('THE LAW', line) != None or re.search('PROCEEDINGS', line) != None:\n",
    "                            break\n",
    "                    facts.append(contline)\n",
    "        for i in range(len(facts)):\n",
    "            D.append((facts[i], violation, years[i]))\n",
    "        \n",
    "    if part == 'facts':\n",
    "        starts = 'THE FACTS'\n",
    "        ends ='THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'circumstances':\n",
    "        starts = 'CIRCUMSTANCES'\n",
    "        ends ='RELEVANT'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'procedure':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends ='THE FACTS'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'procedure+facts':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends ='THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'facts+circumstances':\n",
    "        starts = 'THE FACTS'\n",
    "        ends = 'RELEVANT'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'facts+circumstances+procedure':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends = 'THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cross_val(Xtrain, Ytrain, vec, c): #Linear SVC model cross-validation\n",
    "    print('***10-fold cross-validation***')\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion(\n",
    "            [vec],\n",
    "        )),\n",
    "        ('classifier', LinearSVC(C=c))\n",
    "        ])\n",
    "    Ypredict = cross_val_predict(pipeline, Xtrain, Ytrain, cv=10) #10-fold cross-validation\n",
    "    evaluate(Ytrain, Ypredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_test(Xtrain, Ytrain, Xtest_v, Ytest_v, vec, c): #test on 'violations' test set\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([vec]\n",
    "        )),\n",
    "        ('classifier', XGBClassifier())\n",
    "        ])\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    print('***testing on violation testset***')\n",
    "    Ypredict = pipeline.predict(Xtest_v)\n",
    "    evaluate(Ytest_v, Ypredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Ytest, Ypredict): #evaluate the model (accuracy, precision, recall, f-score, confusion matrix)\n",
    "        print('Accuracy:', accuracy_score(Ytest, Ypredict) )\n",
    "        print('\\nClassification report:\\n', classification_report(Ytest, Ypredict))\n",
    "        print('\\nCR:', precision_recall_fscore_support(Ytest, Ypredict, average='macro'))\n",
    "        print('\\nConfusion matrix:\\n', confusion_matrix(Ytest, Ypredict), '\\n\\n_______________________\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(part, vec, c): #run tests\n",
    "    \n",
    "    print('Trained on *' + part + '* part of the cases')\n",
    "    \n",
    "    v = extract_parts(path+'train/'+article+'/violation/*.txt', 'violation', part)\n",
    "    nv = extract_parts(path+'train/'+article+'/non-violation/*.txt', 'non-violation', part)\n",
    "    trainset =v+nv\n",
    "    shuffle(trainset)\n",
    "\n",
    "    Xtrain = [i[0] for i in trainset]\n",
    "    Ytrain = [i[1] for i in trainset]\n",
    "    \n",
    "    #test set with violations only\n",
    "    if article == 'Article14':\n",
    "        test = extract_parts('./test_violations/'+article+'/*.txt', 'non-violation', part)\n",
    "    else:\n",
    "        test = extract_parts('./test_violations/'+article+'/*.txt', 'violation', part)\n",
    "    Xtest_v = [i[0] for i in test]\n",
    "    Ytest_v = [i[1] for i in test]\n",
    "    \n",
    "\n",
    "    print('Training on', Ytrain.count('violation'),'+', Ytrain.count('non-violation'), '=', Ytrain.count('violation') + Ytrain.count('non-violation'), 'cases', '\\nCases available for testing(violation):', Ytest_v.count('violation'))\n",
    "    #train_model_test(Xtrain, Ytrain, Xtest_v, Ytest_v, vec, c)\n",
    "    train_model_cross_val(Xtrain, Ytrain, vec, c) #use for cross-validation\n",
    "    print(len(v[1]), len(nv[1]), len(Xtrain[1]), len(Ytrain))\n",
    "    return v, nv, Xtrain, Ytrain, Xtest_v, Ytest_v, trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on *facts* part of the cases\n",
      "Training on 56 + 57 = 113 cases \n",
      "Cases available for testing(violation): 0\n",
      "***10-fold cross-validation***\n",
      "Accuracy: 0.7079646017699115\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-violation       0.70      0.74      0.72        57\n",
      "    violation       0.72      0.68      0.70        56\n",
      "\n",
      "     accuracy                           0.71       113\n",
      "    macro avg       0.71      0.71      0.71       113\n",
      " weighted avg       0.71      0.71      0.71       113\n",
      "\n",
      "\n",
      "CR: (0.7084905660377359, 0.7077067669172932, 0.7075982121853681, None)\n",
      "\n",
      "Confusion matrix:\n",
      " [[42 15]\n",
      " [18 38]] \n",
      "\n",
      "_______________________\n",
      "\n",
      "\n",
      "3 3 34059 113\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Path to the data\n",
    "    path = 'C:\\\\Users\\\\35387\\\\Dropbox\\\\Documents\\\\Forecasting\\\\Final_Project\\\\data\\\\'\n",
    "    \n",
    "    articles = ['Article2'] # 'Article3', 'Article5', 'Article6', 'Article8', 'Article10', 'Article11', 'Article13', 'Article14']\n",
    "    for part in ['facts']: #, 'facts+circumstances', 'facts+circumstances+procedure']:\n",
    "        for article in articles: #the parameters were determined using grid-search\n",
    "            vec = ('wordvec', TfidfVectorizer(analyzer = 'word', ngram_range = (3,4), binary = False, lowercase = True, min_df = 2, norm = 'l2', stop_words = None, use_idf = True))\n",
    "            c = 0.1\n",
    "            v, nv, Xtrain, Ytrain, Xtest_v, Ytest_v, trainset = run_pipeline(part, vec, c) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
